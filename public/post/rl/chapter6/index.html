<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Introduction to Reinforcement Learning: Chapter 6 | Myungjae Cho</title>
<meta name="keywords" content="RL">
<meta name="description" content="Credits All contents are summaries of &ldquo;Reinforcement Learning, An Introduction second edition&rdquo; by Richard Sutton and Andrew Barto
Chapter 6: Temporal-Difference Learning 6.1 TD prediction Both TD and Monte Carlo methods use experience (sample trajectory) to solve the prediction problem. Because MC method uses a return $G_t$ as the update target, it needs to wait until the return is known.
MC method constant-$\alpha$ MC method can be represented as
$$V(S_t) \leftarrow V(S_t) &#43; \alpha[G_t - V(S_t)] \tag{6.">
<meta name="author" content="">
<link rel="canonical" href="http://nameontree.github.io/post/rl/chapter6/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.abc7c82c3d415a6df50430738d1cbcc4c76fea558bc5a0c830d3babf78167a35.css" integrity="sha256-q8fILD1BWm31BDBzjRy8xMdv6lWLxaDIMNO6v3gWejU=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="http://nameontree.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://nameontree.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://nameontree.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://nameontree.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="http://nameontree.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Introduction to Reinforcement Learning: Chapter 6" />
<meta property="og:description" content="Credits All contents are summaries of &ldquo;Reinforcement Learning, An Introduction second edition&rdquo; by Richard Sutton and Andrew Barto
Chapter 6: Temporal-Difference Learning 6.1 TD prediction Both TD and Monte Carlo methods use experience (sample trajectory) to solve the prediction problem. Because MC method uses a return $G_t$ as the update target, it needs to wait until the return is known.
MC method constant-$\alpha$ MC method can be represented as
$$V(S_t) \leftarrow V(S_t) &#43; \alpha[G_t - V(S_t)] \tag{6." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://nameontree.github.io/post/rl/chapter6/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2022-07-28T04:38:34-04:00" />
<meta property="article:modified_time" content="2022-07-28T04:38:34-04:00" /><meta property="og:site_name" content="Sprinkle stars" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Introduction to Reinforcement Learning: Chapter 6"/>
<meta name="twitter:description" content="Credits All contents are summaries of &ldquo;Reinforcement Learning, An Introduction second edition&rdquo; by Richard Sutton and Andrew Barto
Chapter 6: Temporal-Difference Learning 6.1 TD prediction Both TD and Monte Carlo methods use experience (sample trajectory) to solve the prediction problem. Because MC method uses a return $G_t$ as the update target, it needs to wait until the return is known.
MC method constant-$\alpha$ MC method can be represented as
$$V(S_t) \leftarrow V(S_t) &#43; \alpha[G_t - V(S_t)] \tag{6."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Introduction to Reinforcement Learning: Chapter 6",
      "item": "http://nameontree.github.io/post/rl/chapter6/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Introduction to Reinforcement Learning: Chapter 6",
  "name": "Introduction to Reinforcement Learning: Chapter 6",
  "description": "Credits All contents are summaries of \u0026ldquo;Reinforcement Learning, An Introduction second edition\u0026rdquo; by Richard Sutton and Andrew Barto\nChapter 6: Temporal-Difference Learning 6.1 TD prediction Both TD and Monte Carlo methods use experience (sample trajectory) to solve the prediction problem. Because MC method uses a return $G_t$ as the update target, it needs to wait until the return is known.\nMC method constant-$\\alpha$ MC method can be represented as\n$$V(S_t) \\leftarrow V(S_t) + \\alpha[G_t - V(S_t)] \\tag{6.",
  "keywords": [
    "RL"
  ],
  "articleBody": "Credits All contents are summaries of “Reinforcement Learning, An Introduction second edition” by Richard Sutton and Andrew Barto\nChapter 6: Temporal-Difference Learning 6.1 TD prediction Both TD and Monte Carlo methods use experience (sample trajectory) to solve the prediction problem. Because MC method uses a return $G_t$ as the update target, it needs to wait until the return is known.\nMC method constant-$\\alpha$ MC method can be represented as\n$$V(S_t) \\leftarrow V(S_t) + \\alpha[G_t - V(S_t)] \\tag{6.1}$$\nwhere $G_t$ is the actual return following time t, and $\\alpha$ is a constant step-size parameter.\nTD(0) or one-step TD Whereas Monte Carlo methods must wait until $G_t$ is known, TD methods can immediately update $V(S_t)$ after the next time step by making use of observed reward $R_{t+1}$ and the estimate $V(S_{t+1})$.\n$$V(S_t) \\leftarrow V(S_t) + \\alpha[R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)] \\tag{6.2}$$\nHere, the update target for TD update is $R_{t+1} + \\gamma V(S_{t+1})$. This TD method is called TD(0), or one-step TD.\nBecause TD(0) makes use of an existing estimate, it is also called as bootstrapping method like DP.\n$$v_{\\pi}(s) = \\mathbb E_{\\pi}[G_t | S_t = s] \\tag{6.3}$$ $$ = \\mathbb E_{\\pi}[R_{t+1} + \\gamma G_{t+1} | S_t = s] \\tag{from 3.9}$$ $$ = \\mathbb E_{\\pi}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t = s] \\tag{6.4}$$\nThe quantity in brackets in the TD(0) update is an error called TD error, measuring the difference between the estimated value of $S_t$ and the better estimate $R_{t+1} + \\gamma V(S_{t+1})$\n$$\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\tag{6.5}$$\nActually, the Monte Carlo error can be written as a sum of TD erros if array V does not change during an episode.\n$$G_t = V(S_t) = R_{t+1} + \\gamma G_{t+1} - V(S_t) + \\gamma V(S_{t+1}) - \\gamma V(S_{t+1}) \\tag{from 3.9}$$ $$ = \\delta_t + \\gamma(G_{t+1} - V(S_{t+1}))$$ $$ = \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2(G_{t+2} - V(S_{t+2}))$$ $$ = \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + … + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t}(G_T - V(S_{T}))$$ $$ = \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + … + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t}(0 - 0)$$ $$ = \\sum_{k=t}^{T-1} \\gamma^{k-t}\\delta_t \\tag{6.6}$$\n6.2 Advantages of TD Prediction Methods  Does not require a model unlike DP methods do Can learn immediately after one time step (TD(0)). Does not need to wait for the full return unlike MC methods must do. Can be also used for continuous tasks. some Monte Carlo methods must discard episodes on which experimental actions are taken. This slows down the learning processes. DP methods do not have these restrictions.  TD methods are guaranteed to converge to $v_{\\pi}$ for any fixed policy ${\\pi}$ There’s still debate on which method has a faster learning speed, but in practice, TD methods converge faster than MC methods.\n6.3 Optimality of TD(0) While MC methods and TD methods are both guaranteed to converge to $v_{\\pi}$ for an infinite amount of sample, usually TD methods give more resonable estimate of the values than MC methods give under a condition of limited experience.\nBatch MC methods always try to minimize mean-squared error of $G_t$ on the training set, but in this way MC methods fails to predict well on the unseen data. On the other hand, batch TD methods minimizes the error so that it can better fit into the maximum likelihood estimate model of the Markov process.\nGiven this model, we can correctly calculate the value if the model is correct. This is called the certainty-equivalence estimate because it is equivalent to assuming that the estimate of the underlying process was known with certainty rather than being approximated.\nIn general, batch TD(0) converges to the certainty-equivalence estimate.\n",
  "wordCount" : "603",
  "inLanguage": "en",
  "datePublished": "2022-07-28T04:38:34-04:00",
  "dateModified": "2022-07-28T04:38:34-04:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://nameontree.github.io/post/rl/chapter6/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Myungjae Cho",
    "logo": {
      "@type": "ImageObject",
      "url": "http://nameontree.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top"><script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$','$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  
    window.addEventListener('load', (event) => {
        document.querySelectorAll("mjx-container").forEach(function(x){
          x.parentElement.classList += 'has-jax'})
      });
  
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://nameontree.github.io" accesskey="h" title="Myungjae Cho (Alt + H)">Myungjae Cho</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Introduction to Reinforcement Learning: Chapter 6
    </h1>
    <div class="post-meta"><span title='2022-07-28 04:38:34 -0400 -0400'>July 28, 2022</span>

</div>
  </header> 
  <div class="post-content"><h1 id="credits">Credits<a hidden class="anchor" aria-hidden="true" href="#credits">#</a></h1>
<p>All contents are summaries of <em><strong>&ldquo;Reinforcement Learning, An Introduction second edition&rdquo; by Richard Sutton and Andrew Barto</strong></em></p>
<h1 id="chapter-6-temporal-difference-learning">Chapter 6: Temporal-Difference Learning<a hidden class="anchor" aria-hidden="true" href="#chapter-6-temporal-difference-learning">#</a></h1>
<h2 id="61-td-prediction">6.1 TD prediction<a hidden class="anchor" aria-hidden="true" href="#61-td-prediction">#</a></h2>
<p>Both TD and Monte Carlo methods use experience (sample trajectory) to solve the prediction problem. Because MC method uses a return $G_t$ as the update target, it needs to wait until the return is known.</p>
<h3 id="mc-method">MC method<a hidden class="anchor" aria-hidden="true" href="#mc-method">#</a></h3>
<p>constant-$\alpha$ MC method can be represented as</p>
<p>$$V(S_t) \leftarrow V(S_t) + \alpha[G_t - V(S_t)] \tag{6.1}$$</p>
<p>where $G_t$ is the actual return following time t, and $\alpha$ is a constant step-size parameter.</p>
<h3 id="td0-or-one-step-td">TD(0) or one-step TD<a hidden class="anchor" aria-hidden="true" href="#td0-or-one-step-td">#</a></h3>
<p>Whereas Monte Carlo methods must wait until $G_t$ is known, TD methods can immediately update $V(S_t)$ after the next time step by making use of observed reward $R_{t+1}$ and the estimate $V(S_{t+1})$.</p>
<p>$$V(S_t) \leftarrow V(S_t) + \alpha[R_{t+1} + \gamma V(S_{t+1}) - V(S_t)] \tag{6.2}$$</p>
<p>Here, the update target for TD update is $R_{t+1} + \gamma V(S_{t+1})$. This TD method is called TD(0), or one-step TD.</p>
<p>Because TD(0) makes use of an existing estimate, it is also called as <em><strong>bootstrapping method</strong></em> like DP.</p>
<p>$$v_{\pi}(s) = \mathbb E_{\pi}[G_t | S_t = s] \tag{6.3}$$
$$ = \mathbb E_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s] \tag{from 3.9}$$
$$ = \mathbb E_{\pi}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s] \tag{6.4}$$</p>
<p>The quantity in brackets in the TD(0) update is an error called <em><strong>TD error</strong></em>, measuring the difference between the estimated value of $S_t$ and the better estimate $R_{t+1} + \gamma V(S_{t+1})$</p>
<p>$$\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \tag{6.5}$$</p>
<p>Actually, the Monte Carlo error can be written as a sum of TD erros if array V does not change during an episode.</p>
<p>$$G_t = V(S_t) = R_{t+1} + \gamma G_{t+1} - V(S_t) + \gamma V(S_{t+1}) - \gamma V(S_{t+1}) \tag{from 3.9}$$
$$ = \delta_t + \gamma(G_{t+1} - V(S_{t+1}))$$
$$ = \delta_t + \gamma \delta_{t+1} + \gamma^2(G_{t+2} - V(S_{t+2}))$$
$$ = \delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2} + &hellip; + \gamma^{T-t-1} \delta_{T-1} + \gamma^{T-t}(G_T - V(S_{T}))$$
$$ = \delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2} + &hellip; + \gamma^{T-t-1} \delta_{T-1} + \gamma^{T-t}(0 - 0)$$
$$ = \sum_{k=t}^{T-1} \gamma^{k-t}\delta_t \tag{6.6}$$</p>
<h2 id="62-advantages-of-td-prediction-methods">6.2 Advantages of TD Prediction Methods<a hidden class="anchor" aria-hidden="true" href="#62-advantages-of-td-prediction-methods">#</a></h2>
<ol>
<li>Does not require a model unlike DP methods do</li>
<li>Can learn immediately after one time step (TD(0)). Does not need to wait for the full return unlike MC methods must do. Can be also used for continuous tasks.</li>
<li>some Monte Carlo methods must discard episodes on which experimental actions are taken. This slows down the learning processes. DP methods do not have these restrictions.</li>
</ol>
<p>TD methods are guaranteed to converge to $v_{\pi}$ for any fixed policy ${\pi}$
There&rsquo;s still debate on which method has a faster learning speed, but in practice, TD methods converge faster than MC methods.</p>
<h2 id="63-optimality-of-td0">6.3 Optimality of TD(0)<a hidden class="anchor" aria-hidden="true" href="#63-optimality-of-td0">#</a></h2>
<p>While MC methods and TD methods are both guaranteed to converge to $v_{\pi}$ for an infinite amount of sample, usually TD methods give more resonable estimate of the values than MC methods give under a condition of limited experience.</p>
<p>Batch MC methods always try to minimize mean-squared error of $G_t$ on the training set, but in this way MC methods fails to predict well on the unseen data. On the other hand, batch TD methods minimizes the error so that it can better fit into the <em><strong>maximum likelihood estimate model</strong></em> of the Markov process.</p>
<p>Given this model, we can correctly calculate the value if the model is correct. This is called the <em><strong>certainty-equivalence estimate</strong></em> because it is equivalent to assuming that the estimate of the underlying process was known with certainty rather than being approximated.</p>
<p>In general, batch TD(0) converges to the certainty-equivalence estimate.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://nameontree.github.io/tags/rl/">RL</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="http://nameontree.github.io">Myungjae Cho</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
