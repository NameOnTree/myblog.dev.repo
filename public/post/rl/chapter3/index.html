<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Introduction to Reinforcement Learning: Chapter 3 | Myungjae Cho</title>
<meta name="keywords" content="RL">
<meta name="description" content="Credits All contents are summaries of &ldquo;Reinforcement Learning, An Introduction second edition&rdquo; by Richard Sutton and Andrew Barto
Chapter 3: Finite Markov Decision Processes 3.0 Introduction  Markov Decision Processes is mathematically idealized form of reinforcement learning There is always a tension between Applicability and Mathematical Tractability, but MDPs offers well generalized solution to many RL problems.  3.1 The Agent-Environment Interface Agent-Environment Interaction in MDPs MDPs (Markov Decision Processes): Framing of a problem of learning &amp; decision making from interaction to acheive a goal.">
<meta name="author" content="">
<link rel="canonical" href="http://nameontree.github.io/post/rl/chapter3/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.abc7c82c3d415a6df50430738d1cbcc4c76fea558bc5a0c830d3babf78167a35.css" integrity="sha256-q8fILD1BWm31BDBzjRy8xMdv6lWLxaDIMNO6v3gWejU=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="http://nameontree.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://nameontree.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://nameontree.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://nameontree.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="http://nameontree.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Introduction to Reinforcement Learning: Chapter 3" />
<meta property="og:description" content="Credits All contents are summaries of &ldquo;Reinforcement Learning, An Introduction second edition&rdquo; by Richard Sutton and Andrew Barto
Chapter 3: Finite Markov Decision Processes 3.0 Introduction  Markov Decision Processes is mathematically idealized form of reinforcement learning There is always a tension between Applicability and Mathematical Tractability, but MDPs offers well generalized solution to many RL problems.  3.1 The Agent-Environment Interface Agent-Environment Interaction in MDPs MDPs (Markov Decision Processes): Framing of a problem of learning &amp; decision making from interaction to acheive a goal." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://nameontree.github.io/post/rl/chapter3/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2022-05-16T13:03:22-04:00" />
<meta property="article:modified_time" content="2022-05-16T13:03:22-04:00" /><meta property="og:site_name" content="Sprinkle stars" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Introduction to Reinforcement Learning: Chapter 3"/>
<meta name="twitter:description" content="Credits All contents are summaries of &ldquo;Reinforcement Learning, An Introduction second edition&rdquo; by Richard Sutton and Andrew Barto
Chapter 3: Finite Markov Decision Processes 3.0 Introduction  Markov Decision Processes is mathematically idealized form of reinforcement learning There is always a tension between Applicability and Mathematical Tractability, but MDPs offers well generalized solution to many RL problems.  3.1 The Agent-Environment Interface Agent-Environment Interaction in MDPs MDPs (Markov Decision Processes): Framing of a problem of learning &amp; decision making from interaction to acheive a goal."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Introduction to Reinforcement Learning: Chapter 3",
      "item": "http://nameontree.github.io/post/rl/chapter3/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Introduction to Reinforcement Learning: Chapter 3",
  "name": "Introduction to Reinforcement Learning: Chapter 3",
  "description": "Credits All contents are summaries of \u0026ldquo;Reinforcement Learning, An Introduction second edition\u0026rdquo; by Richard Sutton and Andrew Barto\nChapter 3: Finite Markov Decision Processes 3.0 Introduction  Markov Decision Processes is mathematically idealized form of reinforcement learning There is always a tension between Applicability and Mathematical Tractability, but MDPs offers well generalized solution to many RL problems.  3.1 The Agent-Environment Interface Agent-Environment Interaction in MDPs MDPs (Markov Decision Processes): Framing of a problem of learning \u0026amp; decision making from interaction to acheive a goal.",
  "keywords": [
    "RL"
  ],
  "articleBody": "Credits All contents are summaries of “Reinforcement Learning, An Introduction second edition” by Richard Sutton and Andrew Barto\nChapter 3: Finite Markov Decision Processes 3.0 Introduction  Markov Decision Processes is mathematically idealized form of reinforcement learning There is always a tension between Applicability and Mathematical Tractability, but MDPs offers well generalized solution to many RL problems.  3.1 The Agent-Environment Interface Agent-Environment Interaction in MDPs MDPs (Markov Decision Processes): Framing of a problem of learning \u0026 decision making from interaction to acheive a goal.\n Agent: Learner and decision maker that interacts with environment Environment: Everything outside of agent  The interaction is characterized by the three discrete or continuous spaces: States, Actions, and Reward.\n  States: Representation of environmnent, denoted by $S_t\\in S$\n  Action: Action based on given State, denoted by $A_t\\in A(s)$\n  Reward: Goal that agent tries to maximize over time, denoted by $R_t\\in R$\n  At each time step t, Agent receives a state, selects an action based on the state, and receives reward.\nOften, this processes is expanded by a series of time steps, which is called Trajectory:\n$$S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, … \\tag{3.1}$$\nDynamics of MDP In finite MDP, sets of $S$, $A$, $R$ all have a finite number of elements, and the corresponding random variables $S$, $R$ have discrete probability distributions.\nA probability distribution $p$ completely characterizes the dynamics of MDP, and is defined by a following equation:\n$$p(s’,r|s,a) = Pr(S_t=s’, R_t=r | S_{t-1}=s, A_{t-1}=a) \\tag{3.2}$$\n$$\\sum_{s’\\in S} \\sum_{r\\in R} p(s’, r|s, a) = 1 \\tag{3.3}$$\nMDP makes an important assumption that the preceding state must includes all information about all aspects of the past agent-environment interaction that makes a difference for the future.\nWhen state follows the above assumption, then the state is called to have Markov property\nIf dynamics $p$ exists, then obtaining different probabilities associated with it is trivial\n$$p(s'|s, a) = \\sum_{r\\in R} p(s’, r|s, a) \\tag{3.4}$$\n$$r(s,a) = E[R_t | S_{t-1}=s, A_{t-1}=a] = \\sum_{r\\in R}r\\sum_{s’\\in S}p(s’, r|s, a) \\tag{3.5}$$\nDistinction between Agent and Environment A boundary between agent and environment is not the same as the physical boundary of a robot’s or animal’s body\nAnything that cannot be changed by agent is considered to be a part of environment.\n3.2 Goals and Rewards   Reward: a simple number $R_t\\in \\mathbb R$, the agent’s goal is to maximize the total amount of reward it receives\n  Reward hypothesis: all of what we mean by goals and purpose can be well though of as the maximization of the expected value of the cumulative sum of a received scalar signal\n  3.3 Returns and Episodes We seek to maximize expected return.\n Return: $G_t$, some specific function of the reward sequence.  In the simplest case, $$G_t = R_{t+1} + R_{t+2} + R_{t+3} + … + R_{T} \\tag{3.7}$$ Where $T$ is a final time step.\nBreaking the whole reward sequence by a final time step especially makes sense when agent-environment interaction naturally breaks into several episodes\n  episodes: any sort of repeated interaction. For example, playing a game several times can be thought of as sampling several episodes. Each episodes terminate in a special state called terminal state, followed by reset to a starting state.\n  episodic tasks: tasks that consist of several episodes\n  continuing tasks: tasks that agent-environment interaction does not naturally break into several episodes, but goes on continually without limit.\n  The previous return fomulation could be a problem in continuing tasks because returns can easily sum up to infinite as $T \\to \\infty$.\nFortunately, discounted return can solve this issue.\n$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + … = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\tag{3.8}$$\nwhere $\\gamma$ is a parameter, $0 \\leq \\gamma \\leq 1$, called the discount rate.\nIf $\\gamma$ gets closer 0, the discounted return is weighted heavily toward the most recent reward. This encourages agent to focus more on recent events. On the other side, if $\\gamma$ gets closer to 1, agent is encouraged to become more farsighted.\n$$\\displaylines{G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4} + … \\\\ = R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3} + \\gamma^2 R_{t+4} + …) \\\\ = R_{t+1} + \\gamma G_{t+1} \\tag{3.9}}$$\nIf the reward is a constant +1, then the return is\n$$G_t = \\sum_{k=0}^\\infty \\gamma^k = \\frac{1}{1-\\gamma} \\tag{3.10}$$\n3.4 Unified Notation for Episodic and Continuing Tasks For simplicity, we can unify the case of episodic and continuing task by considering terminal state to be a special absorbing state that transitions only to itself.\nThis helps us to establish a convention to obtain a single notation that covers both episodic and continuing task.\n$$G_t = \\sum_{k=t+1}^T \\gamma^{k-t-1}R_k \\tag{3.11}$$\nincluding the possiblity that $T = \\infty$ or $\\gamma = 1$ (but not both).\n3.5 Policies and Value Functions Many reinforcement learning algorithms seek to estimate value functions. Value function represents how good a given state is for the agents.\nPolicy and Value Functions policy is a mapping from states to probabilities of selecting each possible action. Mathmatically, for given policy $\\pi$ at time $t$, the mapping is described as $\\pi(a|s)$.\nstate-value function of a state $s$ under a policy $\\pi$, $v_{\\pi}(s)$, is defined as\n$$v_{\\pi}(s) = \\mathbb E_{\\pi}[G_t | S_t = s] = \\mathbb E_{\\pi}[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t = s] \\tag{3.12}$$\nfor all $s \\in \\mathbb S$, where $\\mathbb E_{\\pi}[\\cdot]$ is an expectation of a random variable given that the agent follows policy $\\pi$, and $t$ is any time step.\nSimilarly, action-value function of a state and an action at time $t$ under a policy $\\pi$, $q_{\\pi}(s,a)$, is defined as\n$$q_{\\pi}(s,a) = \\mathbb E_{\\pi}[G_t | S_t = s, A_t = a] = \\mathbb E_{\\pi}[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t = s, A_t = a] \\tag{3.13}$$\nBellman equation A nice thing about these value functions is that they are recursive, meaning that the value function can be represented as the value function itself.\nFor any policy $\\pi$ and any state $s$, the following equation holds between the value of $s$ and the value of its possible successor states:\n$$v_{\\pi} = \\mathbb E_{\\pi}[G_t | S_t = s]$$ $$ = \\mathbb E_{\\pi}[R_{t+1} + \\gamma G_{t+1} | S_t = s]$$ $$ = \\sum_a \\pi(a|s) \\sum_{s’,r} p(s’,r|s,a)[r + \\gamma \\mathbb E_{\\pi}[G_{t+1}|S_{t+1}=s’]]$$ $$ = \\sum_a \\pi(a|s) \\sum_{s’,r} p(s’,r|s,a)[r + \\gamma v_{\\pi}(s’)] \\tag{3.14}$$\nfor all $s \\in \\mathbb S$. Actions, a, are taken from the set $\\mathbb A(s)$ and the next states, $s'$ are taken from the set $\\mathbb S$, and rewards, $r$, are taken from the set $\\mathbb R$\nThe equation 3.14 is Bellman equation for $v_{\\pi}$. Basically, Bellman equation is a recursive definition of the expected discounted return, $v_{\\pi}$. This means $v_{\\pi}(s)$ can be represented as the expression of $v_{\\pi}(s’)$, which makes the value of current state easier to understand in terms of the value of previous state. It states $v_{\\pi}(s)$ is the average of $r + \\gamma v_{\\pi}(s’)$ of the future states, weighted by the possibilities of $\\pi(a|s)$ and $p(s’,r|s,a)$.\nNote that this recursive definition of Bellman equation enables us to build a backup diagram of tree-like structure, and is a convenient method to visualize the overall picture.\n3.6 Optimal Policies and Optimal Value Functions Choosing a policy that always maximizes the value gives us an optimal value functions. $v_{\\star}(s) = \\max_{\\pi} v_{\\pi}(s)\\tag{3.15}$ this can be also written as action-value function $q_{\\star}(s, a) = \\max_{\\pi} q_{\\pi}(s, a)\\tag{3.16}$\ndecomposing the function gives $q_{\\star}(s, a) = \\mathbb E[R_{t+1} + \\gamma v_{\\star}(S_{t+1}) | S_t = s, A_t = a]\\tag{3.17}$\n",
  "wordCount" : "1231",
  "inLanguage": "en",
  "datePublished": "2022-05-16T13:03:22-04:00",
  "dateModified": "2022-05-16T13:03:22-04:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://nameontree.github.io/post/rl/chapter3/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Myungjae Cho",
    "logo": {
      "@type": "ImageObject",
      "url": "http://nameontree.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top"><script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$','$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  
    window.addEventListener('load', (event) => {
        document.querySelectorAll("mjx-container").forEach(function(x){
          x.parentElement.classList += 'has-jax'})
      });
  
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://nameontree.github.io" accesskey="h" title="Myungjae Cho (Alt + H)">Myungjae Cho</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Introduction to Reinforcement Learning: Chapter 3
    </h1>
    <div class="post-meta"><span title='2022-05-16 13:03:22 -0400 -0400'>May 16, 2022</span>

</div>
  </header> 
  <div class="post-content"><h1 id="credits">Credits<a hidden class="anchor" aria-hidden="true" href="#credits">#</a></h1>
<p>All contents are summaries of <em><strong>&ldquo;Reinforcement Learning, An Introduction second edition&rdquo; by Richard Sutton and Andrew Barto</strong></em></p>
<h2 id="chapter-3-finite-markov-decision-processes">Chapter 3: Finite Markov Decision Processes<a hidden class="anchor" aria-hidden="true" href="#chapter-3-finite-markov-decision-processes">#</a></h2>
<h2 id="30-introduction">3.0 Introduction<a hidden class="anchor" aria-hidden="true" href="#30-introduction">#</a></h2>
<ul>
<li>Markov Decision Processes is mathematically idealized form of reinforcement learning</li>
<li>There is always a tension between Applicability and Mathematical Tractability, but MDPs offers well generalized solution to many RL problems.</li>
</ul>
<h2 id="31-the-agent-environment-interface">3.1 The Agent-Environment Interface<a hidden class="anchor" aria-hidden="true" href="#31-the-agent-environment-interface">#</a></h2>
<h3 id="agent-environment-interaction-in-mdps">Agent-Environment Interaction in MDPs<a hidden class="anchor" aria-hidden="true" href="#agent-environment-interaction-in-mdps">#</a></h3>
<p><em><strong>MDPs (Markov Decision Processes):</strong></em> Framing of a problem of learning &amp; decision making from interaction to acheive a goal.</p>
<p><img loading="lazy" src="/InteractionInMDP.png" alt="Agent-Environment Interaction in MDPs"  title="Agent-Environment Interaction in MDPs"  />
</p>
<ul>
<li><em><strong>Agent:</strong></em> Learner and decision maker that interacts with environment</li>
<li><em><strong>Environment:</strong></em> Everything outside of agent</li>
</ul>
<p>The interaction is characterized by the three discrete or continuous spaces: States, Actions, and Reward.</p>
<ol>
<li>
<p><em><strong>States:</strong></em> Representation of environmnent, denoted by $S_t\in S$</p>
</li>
<li>
<p><em><strong>Action:</strong></em> Action based on given State, denoted by $A_t\in A(s)$</p>
</li>
<li>
<p><em><strong>Reward:</strong></em> Goal that agent tries to maximize over time, denoted by $R_t\in R$</p>
</li>
</ol>
<p>At each time step t, Agent receives a state, selects an action based on the state, and receives reward.</p>
<p>Often, this processes is expanded by a series of time steps, which is called <em><strong>Trajectory</strong></em>:</p>
<p>$$S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, &hellip; \tag{3.1}$$</p>
<h3 id="dynamics-of-mdp">Dynamics of MDP<a hidden class="anchor" aria-hidden="true" href="#dynamics-of-mdp">#</a></h3>
<p>In finite MDP, sets of $S$, $A$, $R$ all have a finite number of elements, and the corresponding random variables $S$, $R$ have discrete probability distributions.</p>
<p>A probability distribution $p$ completely characterizes the dynamics of MDP, and is defined by a following equation:</p>
<p>$$p(s&rsquo;,r|s,a) = Pr(S_t=s&rsquo;, R_t=r | S_{t-1}=s, A_{t-1}=a) \tag{3.2}$$</p>
<p>$$\sum_{s&rsquo;\in S} \sum_{r\in R} p(s&rsquo;, r|s, a) = 1 \tag{3.3}$$</p>
<p>MDP makes an important assumption that the preceding state must includes all information about all aspects of the past agent-environment interaction that makes a difference for the future.</p>
<p>When state follows the above assumption, then the state is called to have <em><strong>Markov property</strong></em></p>
<p>If dynamics $p$ exists, then obtaining different probabilities associated with it is trivial</p>
<p>$$p(s'|s, a) = \sum_{r\in R} p(s&rsquo;, r|s, a) \tag{3.4}$$</p>
<p>$$r(s,a) = E[R_t | S_{t-1}=s, A_{t-1}=a] = \sum_{r\in R}r\sum_{s&rsquo;\in S}p(s&rsquo;, r|s, a) \tag{3.5}$$</p>
<h3 id="distinction-between-agent-and-environment">Distinction between Agent and Environment<a hidden class="anchor" aria-hidden="true" href="#distinction-between-agent-and-environment">#</a></h3>
<p>A boundary between agent and environment is not the same as the physical boundary of a robot&rsquo;s or animal&rsquo;s body</p>
<p>Anything that cannot be changed by agent is considered to be a part of environment.</p>
<h2 id="32-goals-and-rewards">3.2 Goals and Rewards<a hidden class="anchor" aria-hidden="true" href="#32-goals-and-rewards">#</a></h2>
<ul>
<li>
<p><em><strong>Reward:</strong></em> a simple number $R_t\in \mathbb R$, the agent&rsquo;s goal is to maximize the total amount of reward it receives</p>
</li>
<li>
<p><em><strong>Reward hypothesis:</strong></em> all of what we mean by goals and purpose can be well though of as the maximization of the expected value of the cumulative sum of a received scalar signal</p>
</li>
</ul>
<h2 id="33-returns-and-episodes">3.3 Returns and Episodes<a hidden class="anchor" aria-hidden="true" href="#33-returns-and-episodes">#</a></h2>
<p>We seek to maximize expected <em><strong>return</strong></em>.</p>
<ul>
<li><em><strong>Return:</strong></em> $G_t$, some specific function of the reward sequence.</li>
</ul>
<p>In the simplest case,
$$G_t = R_{t+1} + R_{t+2} + R_{t+3} + &hellip; + R_{T} \tag{3.7}$$
Where $T$ is a final time step.</p>
<p>Breaking the whole reward sequence by a final time step especially makes sense when agent-environment interaction naturally breaks into several <em><strong>episodes</strong></em></p>
<ul>
<li>
<p><em><strong>episodes:</strong></em> any sort of repeated interaction. For example, playing a game several times can be thought of as sampling several episodes. Each episodes terminate in a special state called <em><strong>terminal state</strong></em>, followed by reset to a starting state.</p>
</li>
<li>
<p><em><strong>episodic tasks:</strong></em> tasks that consist of several episodes</p>
</li>
<li>
<p><em><strong>continuing tasks:</strong></em> tasks that agent-environment interaction does not naturally break into several episodes, but goes on continually without limit.</p>
</li>
</ul>
<p>The previous return fomulation could be a problem in continuing tasks because returns can easily sum up to infinite as $T \to \infty$.</p>
<p>Fortunately, <em><strong>discounted</strong></em> return can solve this issue.</p>
<p>$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + &hellip; = \sum_{k=0}^\infty \gamma^k R_{t+k+1} \tag{3.8}$$</p>
<p>where $\gamma$ is a parameter, $0 \leq \gamma \leq 1$, called the <em><strong>discount rate</strong></em>.</p>
<p>If $\gamma$ gets closer 0, the discounted return is weighted heavily toward the most recent reward. This encourages agent to focus more on recent events. On the other side, if $\gamma$ gets closer to 1, agent is encouraged to become more farsighted.</p>
<p>$$\displaylines{G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + &hellip; \\ = R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+4} + &hellip;) \\ = R_{t+1} + \gamma G_{t+1} \tag{3.9}}$$</p>
<p>If the reward is a constant +1, then the return is</p>
<p>$$G_t = \sum_{k=0}^\infty \gamma^k = \frac{1}{1-\gamma} \tag{3.10}$$</p>
<h2 id="34-unified-notation-for-episodic-and-continuing-tasks">3.4 Unified Notation for Episodic and Continuing Tasks<a hidden class="anchor" aria-hidden="true" href="#34-unified-notation-for-episodic-and-continuing-tasks">#</a></h2>
<p>For simplicity, we can unify the case of episodic and continuing task by considering terminal state to be a special absorbing state that transitions only to itself.</p>
<p>This helps us to establish a convention to obtain a single notation that covers both episodic and continuing task.</p>
<p>$$G_t = \sum_{k=t+1}^T \gamma^{k-t-1}R_k \tag{3.11}$$</p>
<p>including the possiblity that $T = \infty$ or $\gamma = 1$ (but not both).</p>
<h2 id="35-policies-and-value-functions">3.5 Policies and Value Functions<a hidden class="anchor" aria-hidden="true" href="#35-policies-and-value-functions">#</a></h2>
<p>Many reinforcement learning algorithms seek to estimate value functions. <em><strong>Value function</strong></em> represents how good a given state is for the agents.</p>
<h3 id="policy-and-value-functions">Policy and Value Functions<a hidden class="anchor" aria-hidden="true" href="#policy-and-value-functions">#</a></h3>
<p><em><strong>policy</strong></em> is a mapping from states to probabilities of selecting each possible action. Mathmatically, for given policy $\pi$ at time $t$, the mapping is described as $\pi(a|s)$.</p>
<p><em><strong>state-value function</strong></em> of a state $s$ under a policy $\pi$, $v_{\pi}(s)$, is defined as</p>
<p>$$v_{\pi}(s) = \mathbb E_{\pi}[G_t | S_t = s] = \mathbb E_{\pi}[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s] \tag{3.12}$$</p>
<p>for all $s \in \mathbb S$, where $\mathbb E_{\pi}[\cdot]$ is an expectation of a random variable given that the agent follows policy $\pi$, and $t$ is any time step.</p>
<p>Similarly, <em><strong>action-value function</strong></em> of a state and an action at time $t$ under a policy $\pi$, $q_{\pi}(s,a)$, is defined as</p>
<p>$$q_{\pi}(s,a) = \mathbb E_{\pi}[G_t | S_t = s, A_t = a] = \mathbb E_{\pi}[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s, A_t = a] \tag{3.13}$$</p>
<h3 id="bellman-equation">Bellman equation<a hidden class="anchor" aria-hidden="true" href="#bellman-equation">#</a></h3>
<p>A nice thing about these value functions is that they are recursive, meaning that the value function can be represented as the value function itself.</p>
<p>For any policy $\pi$ and any state $s$, the following equation holds between the value of $s$ and the value of its possible successor states:</p>
<p>$$v_{\pi} = \mathbb E_{\pi}[G_t | S_t = s]$$
$$ = \mathbb E_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s]$$
$$ = \sum_a \pi(a|s) \sum_{s&rsquo;,r} p(s&rsquo;,r|s,a)[r + \gamma \mathbb E_{\pi}[G_{t+1}|S_{t+1}=s&rsquo;]]$$
$$ = \sum_a \pi(a|s) \sum_{s&rsquo;,r} p(s&rsquo;,r|s,a)[r + \gamma v_{\pi}(s&rsquo;)] \tag{3.14}$$</p>
<p>for all $s \in \mathbb S$. Actions, a, are taken from the set $\mathbb A(s)$ and the next states, $s'$ are taken from the set $\mathbb S$, and rewards, $r$, are taken from the set $\mathbb R$</p>
<p>The equation 3.14 is <em><strong>Bellman equation</strong></em> for $v_{\pi}$.
Basically, Bellman equation is a recursive definition of the expected discounted return, $v_{\pi}$. This means $v_{\pi}(s)$ can be represented as the expression of $v_{\pi}(s&rsquo;)$, which makes the value of current state easier to understand in terms of the value of previous state. It states $v_{\pi}(s)$ is the average of $r + \gamma v_{\pi}(s&rsquo;)$ of the future states, weighted by the possibilities of $\pi(a|s)$ and $p(s&rsquo;,r|s,a)$.</p>
<p>Note that this recursive definition of Bellman equation enables us to build a backup diagram of tree-like structure, and is a convenient method to visualize the overall picture.</p>
<h2 id="36-optimal-policies-and-optimal-value-functions">3.6 Optimal Policies and Optimal Value Functions<a hidden class="anchor" aria-hidden="true" href="#36-optimal-policies-and-optimal-value-functions">#</a></h2>
<p>Choosing a policy that always maximizes the value gives us an optimal value functions.
$v_{\star}(s) = \max_{\pi} v_{\pi}(s)\tag{3.15}$
this can be also written as action-value function
$q_{\star}(s, a) = \max_{\pi} q_{\pi}(s, a)\tag{3.16}$</p>
<p>decomposing the function gives
$q_{\star}(s, a) = \mathbb E[R_{t+1} + \gamma v_{\star}(S_{t+1}) | S_t = s, A_t = a]\tag{3.17}$</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://nameontree.github.io/tags/rl/">RL</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="http://nameontree.github.io">Myungjae Cho</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
